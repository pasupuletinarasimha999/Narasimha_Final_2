AWS Lambda (Memory 128 mb to 10GB, 1000 executions simultaneously, 15 mins timeout)
=========
Can Integration with: Cloudfront, S3, dynamodb, cloudwatch, sns,ses, api gateway, kinesis, cloudtrail

* it runs in public cloud, to avoid running it in public cloud, we can add it to run in our VPC in config of function
we can add all dependencies in single file and upload to layers in aws lambda, then we can add that layer to specific function in lambda
Creation:
	1) Application Name
	2) Runtime: Nodejs, java,python,ruby,go
	3) Processor : arm64 or x86_64
	4) Permissions: IAM Role containing Cloud watch, s3
TABS:
-----
Code source: We need to provide functions to be performed by Lambda (python, node js) directly we can edit here. java,dotnet we need to upload as deployment packages

	handler: lambda_function.lambda_handler ==>like main function in c
change this to index.lamba_handler (index is file name of index.py. default we will get lambda_function file) lamda_handler is function name, we can change this in code and same can be hardcorded in configuration

Test: we need to provide our input values to code here
Logs: clouwatch logs (we will get logs related to specific application)
Configurations: It has role permissions, 

=================================================================
AWS Direct Connect: easy to establish a dedicated network connection from your premises to AWS. can establish private connectivity between AWS and your data center, office, or colocation environment.

Direct Connect Gateway: allows you to connect your AWS Direct Connect connection to one or more VPCs in your account that are located in any AWS Region 
AWS Direct Connect Configuration Steps
	Create a Direct Connect Connection:
		Sign in to the AWS Management Console.
		Navigate to the AWS Direct Connect console.
		Choose "Create a connection."
		Select the location and port speed.
		Complete the connection request.
	Download the Letter of Authorization and Connecting Facility Assignment (LOA-CFA):
		Once the connection request is approved, download the LOA-CFA from the console.
		Provide the LOA-CFA to your network provider to establish the physical connection.
	Configure Your On-Premises Router:
		Configure your on-premises router with the details provided by AWS.
		Ensure that the router is set up to handle BGP (Border Gateway Protocol) sessions.
	Create a Virtual Interface:
		In the AWS Direct Connect console, choose "Create virtual interface."
		Select either a private or public virtual interface.
		Enter the necessary details, such as VLAN ID, BGP ASN, and IP addresses.
	Verify the Connection:
		Ensure that the BGP session is established.
		Verify that you can route traffic between your on-premises network and AWS.

AWS Direct Connect Gateway Configuration Steps
	Create a Direct Connect Gateway:
		In the AWS Direct Connect console, choose "Direct Connect Gateways."
		Choose "Create Direct Connect Gateway."
		Enter a name and ASN for the gateway.
	Associate the Direct Connect Gateway with a Virtual Private Gateway:
		In the Direct Connect console, choose "Virtual Interfaces."
		Select the virtual interface you want to associate.
		Choose "Associate with Direct Connect Gateway."
		Select the Direct Connect Gateway and the Virtual Private Gateway (VGW) to associate.
	Configure Routing:
		Ensure that the appropriate routes are propagated to your on-premises network.
		Verify that traffic can flow between your on-premises network and the VPCs associated with the Direct Connect Gateway.

AWS Transit Gateway: is a network transit hub that you can use to interconnect your VPCs and on-premises networks.
Create a Transit Gateway:
	In the VPC console, choose "Transit Gateways."
	Choose "Create Transit Gateway."
	Enter the necessary details, such as name, description, and ASN.
Create Transit Gateway Attachments:
	In the Transit Gateway console, choose "Create Transit Gateway Attachment."
	Select the resource type (VPC, VPN, Direct Connect Gateway).
	Select all the VPC's which need to be tagged to transit gateway
	Enter the necessary details and create the attachment.
Configure Route Tables:
	In the Transit Gateway console, choose "Route Tables."
	Create a route table and associate it with the appropriate attachments.
	Add routes to direct traffic between the attachments.
Verify Connectivity:
	Ensure that the routes are correctly configured.
	Verify that traffic can flow between the attached VPCs, on-premises networks, and other resources.
===================================================================
VPC Endpoints
--------------
Connects AWS Services privately from VPC instead of going through internet through NIC

Types:
-----
	Interface Endpoints --> all other services (86)
	Gateway Endpoints --> S3 and Dynamodb
	Gateway Loadbalancer Endpoints

VPC -> Endpoints -> Create
	Search for Service
	select VPC to which this service should be accessible
	Select subnets in it
	Select security Groups
	Select Policy (Json File)
	Create Endpoint
=====================================================================
Endpoint service
================
Name, Loadbalancer (Network or gateway), require acceptance for endpoint

Commands in CLI
	aws s3 ls --> this will list all s3 buckets going through Public internet
	aws s3 ls --endpoint-url https://bucket.dns --region ap-south-1 ---> this directly connect endpoint instead of public internet

==========================================================
Cloud Formation -->creates stack of aws resources. we need to create template
------------------
Languages : Yaml or Json
AWSCloudFormationVersion: 2010-09-09
Description:
Resources: 
	MyS3Bucket:
		Type: AWS::S3::Bucket
		Properties
			BucketName: "Name"
	MySecondS3bucket:
		Type: AWS::S3::Bucket
		Properties
			BucketName: "Name"
=====================================================
Types of Storage
------------------
1) DAS - Direct attached storage -> like pendrive
2) NAS - Network attached storage -> storage attached to LAN
3) SAN - Storage attached Network -> Servers attached to Storage LAN
Storage Formats
	File Level (DAS, NAS)
		1) EFS - Linux - NFS
		2) FSX - Windows - SMB
	Block Level (SAN) - EBS
	Object Storage : S3

=============================================

================================================
S3
---
Storage Types
	1) Standard - >3 , frequently accessed
	2) Reduced redundancy - >3, freqeuntly accessed - noncritical
	3) Intelligent Tiering - >3, min 30 days, unknown pattern
	4) Standard IA - >3, min 30 days, infrequently accessed
	5) One-Zone-IA - 1, min 30 days, infrequently accessed
	6) Glacier - >3 , min 90 days
	7) Deep glacier - >3, min 180 days
	
Uploading Object
	Upload the objects
	select storage
	encryption
	
EVERY Object has ACLs - Object owner, public access, authenticated access

Policies which we can create in AWS - SQS, VPC Endpoint, IAM, SNS, S3 Bucket policy

========================================================
Bitbucket
====

Pipeline1.yaml: 
-----------------
		Pipeline:
			default:
				-step:
					script:

			branches:
				master:
     				 - step: *build-and-push-step
    				 - step: *deploy-staging-step
			tags:
				release-*:
     				 - step: *build-and-push-step
   				 - step: *deploy-prod-step
			bookmarks:
			pull requests:
			custom:
				- variables

Pipeline2.yaml: 
-----------------

image: NAME
definitions:
	steps:
		- step: &METHODNAME1
			name:
			caches:
			services
			scripts:
		- step: &METHODNAME2
			name:
			caches:
			services:
			script:
pipelines:
	default:
		- step: *METHODNAME3
		- parallel:
			- step: *METHODNAME1
			- step: *METHODNAME2
	branches:
		master:
			- step: *METHODNAME3
			- parallel:
				- step: *METHODNAME1
				- step: *METHODNAME2
		
	 custom:
    		deploy-mailer:
      		- step:
         			 name: Build and push Mailer to DockerHub
        			 script:
           				 - bash ./ci-scripts/docker-release.sh mailer ./services/mailer/docker/prod/Dockerfile ./services/mailer/package.json
        			 services:
          				 - docker
	
=======================================================
CDK
====

Lifecycle: Construct -> Prepare -> Validate -> Synthesize -> Deploy


=======================================
GuardDuty
==========
Monitors all logs of CloudTrail, VPC Flow Logs, DNS Logs, S3 Data Events, Kubernetes Audit Logs and provides threat details and suspicious things along with their priorities
===========================================================
Code Build

	Project Name, Source Provider (Codecommit, S3, GITHUB, BITBUCKET), repository, reference type (Branch/commit id, git tag), Environment (Managed Image/custom docker image), Service role, Build Timeout, VPC, Compute, Buildspec, Artifacts (S3), cloudwatch

	Buildspec
	-------------
	version: 0.2

environment_variables:
  plaintext:
    PHASE: "build"
    PROJECT: "feh-2018"
    AWS_DEFAULT_REGION: "us-west-2"

phases:
  install:
    runtime-versions:
      docker: 18
      python: 3.7
  pre_build:
    commands:
      - npm install
  build:
    commands:
      - npm test
  post_build:
    commands:

artifacts:
  type: zip
  files:
    - _book/**/*
    - appspec.yml
  discard-paths: no

Code deploy
	Create application (Name, AWS Lambda/ECS/EC2)
	Create deployment group - IAM Role, Deployment strategy (INplace/bluegreen ), Deployment settings (allatonce/halfatonce,oneattime),loadbalancer, instances, triggers(only SNS), alarms, rollback (deployment fails)
	Create deployment - Select application, Deployment group, application from s3
	
		Appsec.yaml
			version: 0.0
			os: linux
			files:
			- source: /index.html
			  destination: /var/www/html
			hooks:
			 ApplicationStop:
				- location: scripts/stop_server.sh
				  timeout: 300
				  runas: root
		     AfterInstall:
				- location: scripts/after-install.sh
				  timeout: 300
				  runas: root
		     BeforeInstall:
				- location: scripts/install_dependies.sh
				  timeout: 300
				  runas: root
			 ApplicationStart:
				- location: scripts/start_server.sh
				  timeout: 300
				  runas: root
			validateserver:
				 - location: scripts/validateserver
				   timeout: 300
		    Beforeblocktraffic:
			BlockTraffic
			AfterBlockTraffic
			BeforeAllowTraffic
			AllowTraffic
			AfterAllowTraffic
			
	
CodePipeline

	Name
	Servicerole
	Artifact Store (s3 location)
	Encryption Key(AWS Managed/Customer Managed)
	Source Provider (Code commit, ECR,S3,Github)
		1)CodeCommit: Repo Name, Branch, change detection options (Cloudwatch events/CodePipeline)
	Build Provider (Codebuild, Jenkins)
	Deploy (Cloud formation, Code deploy,ECS, S3, Beanstalk)
		1) Codedeploy: Region, App Name, Deployment Group)
		
	Usecases
		CodePipeline with S3,CodeCommit,CodeDeploy
		CodePiepline with ThirdParty (Github,Jenkins)
		CodePipeline with AWS CodeStar to Build a pipeline in a code Project
		CodePipeline to complie, Build and Test Code with CodeBuild
		CodePipeline with Amazon ECS for continous Delivery of Container Based Applications to the cloud
		CodePipeline with Beanstalk for continous delivery of Web Applications to the cloud
		Use cloudPipeline with Lambda 
		CodePipeline with CloudFomration

===============================================================

how to integrate my sqs with lambda
Integrating Amazon SQS with AWS Lambda allows you to process messages from your SQS queue automatically using a Lambda function. This integration is useful for building serverless applications that can scale automatically and handle asynchronous tasks efficiently.

Step-by-Step Guide to Integrate SQS with Lambda
1. Create an SQS Queue
	If you haven't already created an SQS queue, follow these steps:

	Sign in to AWS Management Console:
	Go to the AWS Management Console.
	Sign in with your credentials.
	Navigate to SQS:
	In the AWS Management Console, type "SQS" in the search bar and select "Simple Queue Service".
	Create a New Queue:
	Click on "Create queue".
	Choose the type of queue: Standard or FIFO (First-In-First-Out).
	Enter the queue name.
	Configure additional settings like visibility timeout, message retention period, etc.
	Click "Create Queue".
2. Create a Lambda Function
	Navigate to Lambda:
	In the AWS Management Console, type "Lambda" in the search bar and select "Lambda".
	Create a New Function:
	Click on "Create function".
	Choose "Author from scratch".
	Enter a function name.
	Choose a runtime (e.g., Python, Node.js).
	Click "Create function".
	Add Code to the Lambda Function:
In the function code editor, add the code to process messages from the SQS queue.

		import json

		def lambda_handler(event, context):
		for record in event['Records']:
			# Process each message
			message_body = record['body']
			print(f"Received message: {message_body}")
			# Add your message processing logic here

		return {
			'statusCode': 200,
			'body': json.dumps('Messages processed successfully')
		}
3. Add SQS Trigger to Lambda Function
	Configure Triggers:
	In the Lambda function configuration page, click on "Add trigger".
	Select "SQS" from the list of trigger options.
	Configure SQS Trigger:
	Select the SQS queue you created earlier.
	Configure the batch size (the number of messages to process in a single invocation).
	Click "Add".
4. Set Up Permissions
	Lambda needs permission to read messages from the SQS queue. AWS automatically creates the necessary permissions when you add the SQS trigger to the Lambda function. However, you can also manually configure the permissions using IAM policies.

Example IAM Policy:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sqs:ReceiveMessage",
      "Resource": "arn:aws:sqs:us-east-1:123456789012:YourQueueName"
    },
    {
      "Effect": "Allow",
      "Action": "sqs:DeleteMessage",
      "Resource": "arn:aws:sqs:us-east-1:123456789012:YourQueueName"
    },
    {
      "Effect": "Allow",
      "Action": "sqs:GetQueueAttributes",
      "Resource": "arn:aws:sqs:us-east-1:123456789012:YourQueueName"
    }
  ]
}
Attach this policy to the Lambda execution role.

5. Test the Integration
	Send a Test Message to the SQS Queue:
	Navigate to the SQS console.
	Select your queue and click on "Send and receive messages".
	Send a test message to the queue.
	Verify Lambda Execution:
	Navigate to the Lambda console.
	Select your Lambda function and check the "Monitoring" tab to see if the function was invoked.
	Check the logs in Amazon CloudWatch to verify that the message was processed.
		
=====================================================================================
create trust policy and iam role for sts assumerole
Step-by-Step Guide
1. Create a Trust Policy
A trust policy specifies which entities (users, groups, roles, or AWS services) are allowed to assume the role. This policy is attached to the IAM role.

Example Trust Policy:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:role/TrustedRole"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
In this example, only the TrustedRole in the specified AWS account (123456789012) can assume the role.

2. Create an IAM Role
	Sign in to AWS Management Console:
	Go to the AWS Management Console.
	Sign in with your credentials.
	Navigate to IAM:
	In the AWS Management Console, type "IAM" in the search bar and select "IAM".
	Create a New Role:
	Click on "Roles" in the left-hand menu.
	Click on "Create role".
	Select Trusted Entity:
	Choose "Another AWS account".
	Enter the AWS account ID of the trusted account (e.g., 123456789012).
	Attach Policies:
	Attach the necessary policies to the role. For example, if the role needs to access S3, you can attach the AmazonS3ReadOnlyAccess policy.
	Set Permissions Boundary (Optional):
	You can set a permissions boundary to limit the maximum permissions this role can have.
	Review and Create Role:
	Review the role details and click "Create role".

3. Select the User created and add permissions button -> select the role created above
4. copy the assumerole url inside the role created and after switching to above user in aws login, paste the url to switch the role of that user to access the resource
=============================================================
AssumeRoleWithSAML --> AD Login, okta etc

1. Set Up the IdP:
	Configure your SAML 2.0-compliant IdP (e.g., Active Directory Federation Services, Okta, OneLogin) to authenticate users and provide SAML assertions.
2. Export the IdP Metadata:
	Export the IdP metadata XML file, which contains information about the IdP, including its SAML endpoints and certificates.
3. Add a New Identity Provider
	Go to the AWS Management Console.
	Sign in with your credentials.
	In the AWS Management Console, type "IAM" in the search bar and select "IAM".
	Click on "Identity providers" in the left-hand menu.
	Click on "Add provider".
	Select "SAML" as the provider type.
	Provide a name for the provider (e.g., MySAMLProvider).
	Upload the IdP metadata XML file.
	Click "Add provider".
4. Create an IAM Role for SAML Federation
	Click on "Roles" in the left-hand menu.
	Click on "Create role".
	Select "SAML 2.0 federation" as the trusted entity.
	Choose the SAML provider you created (e.g., MySAMLProvider).
	Select the "Allow programmatic and AWS Management Console access" option.
	Attach the necessary policies to the role (e.g., AmazonS3ReadOnlyAccess).
	Review and create the role.

	Trust Policy looks like this:
		{
			"Version": "2012-10-17",
			"Statement": [
				{
				"Effect": "Allow",
				"Principal": {
					"Federated": "arn:aws:iam::123456789012:saml-provider/MySAMLProvider"
				},
				"Action": "sts:AssumeRoleWithSAML",
				"Condition": {
					"StringEquals": {
					"SAML:aud": "https://signin.aws.amazon.com/saml"
					}
				}
				}
			]
			}
5. Configure the IdP to Use the IAM Role
	Configure the IdP to include the necessary assertions in the SAML response to allow users to assume the IAM role.
	Ensure that the SAML assertion includes the RoleArn and PrincipalArn attributes.

Example Code:

		import boto3

		# Initialize STS client
		sts_client = boto3.client('sts')

		# Assume a role with SAML
		response = sts_client.assume_role_with_saml(
		RoleArn='arn:aws:iam::123456789012:role/MySAMLRole',
		PrincipalArn='arn:aws:iam::123456789012:saml-provider/MySAMLProvider',
		SAMLAssertion='BASE64_ENCODED_SAML_ASSERTION',
		DurationSeconds=3600  # 1 hour
		)

		# Extract temporary credentials
		credentials = response['Credentials']
		access_key = credentials['AccessKeyId']
		secret_key = credentials['SecretAccessKey']
		session_token = credentials['SessionToken']

		# Use the temporary credentials to create a new session
		session = boto3.Session(
		aws_access_key_id=access_key,
		aws_secret_access_key=secret_key,
		aws_session_token=session_token
		)

		# Use the session to interact with AWS services
		s3_client = session.client('s3')
		response = s3_client.list_buckets()
		print(response)

==================================================
AssumeRoleWithWebIdentity

1. Configure the Web Identity Provider
	Configure your web identity provider (e.g., Amazon Cognito, Google, Facebook) to authenticate users and provide web identity tokens.
2. Create an IAM Identity Provider in AWS
	Go to the AWS Management Console.
	Sign in with your credentials.
	In the AWS Management Console, type "IAM" in the search bar and select "IAM".
	Click on "Identity providers" in the left-hand menu.
	Click on "Add provider".
	Select "OpenID Connect" as the provider type.
	Provide the necessary details (e.g., provider URL, client ID).
	Click "Add provider".
3. Create an IAM Role for Web Identity Federation
	Click on "Roles" in the left-hand menu.
	Click on "Create role".
	Select "Web identity" as the trusted entity.
	Choose the web identity provider you created.
	Select the "Allow programmatic and AWS Management Console access" option.
	Attach the necessary policies to the role (e.g., AmazonS3ReadOnlyAccess).
	Review and create the role.

	Trust policy looks like this:
	{
		"Version": "2012-10-17",
		"Statement": [
			{
			"Effect": "Allow",
			"Principal": {
				"Federated": "cognito-identity.amazonaws.com"
			},
			"Action": "sts:AssumeRoleWithWebIdentity",
			"Condition": {
				"StringEquals": {
				"cognito-identity.amazonaws.com:aud": "us-east-1:EXAMPLE-IDENTITY-POOL-ID",
				"cognito-identity.amazonaws.com:sub": "EXAMPLE-USER-ID"
				}
			}
			}
		]
		}
4. Configure the Web Identity Provider to Use the IAM Role
	Configure the IdP to include the necessary claims in the web identity token to allow users to assume the IAM role.
	Ensure that the token includes the aud and sub claims.


Sample code for sts inclusion (3 methods)
=============================
sts_client = boto3.client('sts')

# Assume a role with SAML
response = sts_client.assume_role_with_saml(
  RoleArn='arn:aws:iam::123456789012:role/MySAMLRole',
  PrincipalArn='arn:aws:iam::123456789012:saml-provider/MySAMLProvider',
  SAMLAssertion='BASE64_ENCODED_SAML_ASSERTION',
  DurationSeconds=3600  # 1 hour
)

or

response = sts_client.assume_role(
  RoleArn='arn:aws:iam::123456789012:role/MyAssumeRole',
  RoleSessionName='MySession'
)

or

response = sts_client.assume_role_with_web_identity(
  RoleArn='arn:aws:iam::123456789012:role/MyWebIdentityRole',
  RoleSessionName='MySession',
  WebIdentityToken='WEB_IDENTITY_TOKEN',
  DurationSeconds=3600  # 1 hour
)
==========================================================
GetFederationToken: This method is typically used when you want to grant temporary access to AWS resources to users who are authenticated by an external identity provider (IdP) but do not have an IAM user in your AWS account.

import boto3

# Initialize STS client
sts_client = boto3.client('sts')

# Get federation token
response = sts_client.get_federation_token(
  Name='FederatedUser',
  Policy='''{
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": "s3:ListBucket",
              "Resource": "arn:aws:s3:::example-bucket"
          }
      ]
  }''',
  DurationSeconds=3600  # 1 hour
)

# Extract temporary credentials
credentials = response['Credentials']
access_key = credentials['AccessKeyId']
secret_key = credentials['SecretAccessKey']
session_token = credentials['SessionToken']

# Use the temporary credentials to create a new session
session = boto3.Session(
  aws_access_key_id=access_key,
  aws_secret_access_key=secret_key,
  aws_session_token=session_token
)

# Use the session to interact with AWS services
s3_client = session.client('s3')
response = s3_client.list_buckets()
print(response)
===================================================================

AWS Service control policies= AWS Organizations is a service that helps you centrally manage and govern your environment as you grow and scale your AWS resources. It allows you to create multiple AWS accounts and manage them from a single location. One of the key features of AWS Organizations is the use of Service Control Policies (SCPs) to manage permissions across your accounts.

SCPs are a type of policy that you can use to manage permissions in your AWS Organization. They allow you to set permission guardrails on AWS accounts, Organizational Units (OUs), or the entire organization. SCPs do not grant permissions themselves but restrict what permissions can be granted by IAM policies

SCP for Production OU
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:*",
        "s3:*"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Deny",
      "Action": "s3:DeleteBucket",
      "Resource": "*"
    },
    {
      "Effect": "Deny",
      "Action": [
        "ec2:TerminateInstances",
        "s3:DeleteObject"
      ],
      "Resource": "*",
      "Condition": {
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": "false"
        }
      }
    }
  ]
}

SCP for Development OU
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "*",
      "Resource": "*"
    },
    {
      "Effect": "Deny",
      "Action": [
        "iam:CreateRole",
        "iam:DeleteRole"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Deny",
      "Action": "*",
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "aws:RequestedRegion": [
            "us-east-1",
            "us-west-2"
          ]
        }
      }
    }
  ]
}

SCP for Security OU
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "cloudtrail:*",
        "config:*"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Deny",
      "Action": "*",
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "aws:RequestedRegion": [
            "us-east-1",
            "us-west-2"
          ]
        }
      }
    }
  ]
}
Applying SCPs
	Sign in to AWS Management Console:
	Go to the AWS Management Console.
	Sign in with your credentials.
	Navigate to AWS Organizations:
	In the AWS Management Console, type "Organizations" in the search bar and select "AWS Organizations".
Create and Attach SCPs:
	Click on "Policies" in the left-hand menu.
	Click on "Create policy" and enter the JSON for each SCP.
	Name and describe the policy, then click "Create policy".
	Navigate to the "Organizational units" or "Accounts" section.
	Select the OU or account to which you want to attach the SCP.
	Click on "Service control policies" and then "Attach policy".
	Select the SCP you created and click "Attach".

